\chapter{Introduction}

\section{Current Situation and Problems}

Today is the era of big data, there are endless data generated every day, and most of them are heterogeneity on datasets, formats, granularity, so it is hard for machine-read and analyzing. That is the reason scientists and research institutions have come up with the concept of semantic web, whose goal is to make Internet data machine-readable. Semantic web helps to store, manage and share knowledge more efficiently so that computer can use such resources without human effort on parsing the data. On semantic web, the most common framework is The Resource Description Framework (RDF), which is the standard data model representation for knowledge on the Web of Data. RDF is a quite effective way to process and link data, even if the schemas are different. It uses Internationalized Resource Identifier (IRI) to extend the graph, which is a directed, labelled graph, and its edge represent named links between two resources. 

The number of datasets published on the web is constantly increasing(e.g., LinkedGeoData with more than 30 billion triples). Besides, the Linked Data paradigm is based on the unconstrained publication of information by different publishers, and the interlinking of Web resources across knowledge bases. It is not difficult to imagine that there are a lot of redundant and duplicate RDFs here, in order to make our semantic network more efficient and concise, we need to link RDFs together according specific rules, which is the goal of this project.



\section{The Linking Task}

Actually, in most cases, it is really hard to link the cross-dataset, and we must be need using Instance Matching (IM) and Link Discovery tools according a given context. Indeed, here are few engines implemented this goal, like Limes and Silk. But most of them usually implementations rely on their link rule language, so this makes it impossible to use the same link rule in two or more engines. As a result, currently, users find it difficult to use these implementations but the RDF linking is very common tasks need to be finished. Besides, RDF linking is a computational expensive task since it has to compare all the resources form one dataset, with all the resources from another. Then, for each RDF resource one or more data comparisons must be performed. A result, the way in which link rules are expressed (language), built (no of comparisons, restrictions to reduce the number of RDF resources to be compared, etc.), and processed has a high impact on the performance results.

For this reason, we create a more general way to linking ontologies based on SPARQL by implementing functions in Apache Jena, which is a SPARQL query engine. So as a software engineer, you don't need to learn other specific language to linking ontology you want, the SPARQL query is enough.






\newpage


------------------------ instructions for the template -----------------------------


%%---------------------------------------------------------
The introduction of the TFM should serve so that teachers who evaluate the work can understand the context in which it is carried out, and the objectives that are set.

This template shows the basic structure of the final TFM memory, as well as some formatting instructions.
The basic schema of a final  TFM memory is:
\begin{itemize}
\item[•] Summary in Spanish and English (max 2 pages each)
\item[•] Content Table
\item[•] Introduction (with TFM's goals)
\item[•] Master’s Project Content
\item[•] Results and conclusions
\item[•] Bibliography (publications used in the study and work development)
\item[•] Annex (optional)
\end{itemize}


In any case, it is the tutor of the TFM who will indicate to his student the final memory structure that best fits the work developed.

Regarding the format, the following guidelines will be followed, which are shown in this template:
\begin{itemize}
\item[•] \textit{Paper size:} DIN A4
\item[•] \textit{Cover Page:} as stated in this template, with indication of university, center, TFM title and author.
\item[•] \textit{Second page:} bibliographic information, including all data of the TFM tutor.
\item[•] \textit{Letter type for text.} Preferably "Bookman Old Style" 11 points. If this is not possible, the recommended alternatives are, in order of preference: "Palatino Linotype", "Garamond" or "Georgia".
\item[•] \textit{Letter type for source code:} “Consolas” or “Roboto mono”
\item[•] \textit{Margins:} upper and lower $3$ cm, left and $2.54$ cm right.
\item[•] \textit{Sections and subsections:} reviewed with decimal numbering after the chapter number. Ej.: subsections 2.3.1.
\item[•] \textit{Page numbers:} always centered on the lower margin, page 1 begins in chapter 1, all sections before chapter 1 in lowercase Roman numeral (i, ii, iii…).
\end{itemize}

\vspace*{1.5cm}
To prepare the final memory of the TFM with this template, follow the steps below:
\begin{enumerate}
\item Download and Install MiKTeX:  \url{https://miktex.org/}
\item Download and install a \LaTeX~ editor, for example Texmaker:\\
\url{https://www.xm1math.net/texmaker/}

\item Edit the file \textbf{secciones/ \_DatosTFM.tex}, which is included in the folder \textbf{secciones} of this template. Fill in all the requested data in said file. Save and close
\item Compile the file
 \textbf{plantilla\_TFM.tex} (can be renamed). A file \textbf{pdf} will be generated as a result.
\item To write the final memory of the TFM you can add and / or modify the files in the \textbf{secciones} as necessary. The result is obtained when compiling the file \textbf{plantilla\_TFM.tex}. 
\end{enumerate}



\section{Source Code example in Python}
\begin{lstlisting}[style=Python]
# -*- coding: utf-8 -*-
import sympy as sy
from sympy.abc import x
\end{lstlisting}
%%---------------------------------------------------------