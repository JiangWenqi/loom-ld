\chapter{Introduction}

\section{Current Situation and Problems}

Today is the era of big data, there are endless data generated every day, and most of them are heterogeneity on datasets, formats, granularity, so it is hard for machine-read and analyzing. That is the reason scientists and research institutions have come up with the concept of semantic web, whose goal is to make Internet data machine-readable. Semantic web helps to store, manage and share knowledge more efficiently so that computer can use such resources without human effort on parsing the data. On the semantic web, the most common framework is The Resource Description Framework (RDF), which is the standard data model representation for knowledge on the Web of Data. RDF is a quite effective way to process and link data, even if the schemas are different. It uses Internationalized Resource Identifier (IRI) to extend the graph, which is a directed, labelled graph, and its edge represent named links between two resources. 

The number of datasets published on the web is constantly increasing(e.g., LinkedGeoData with more than 30 billion triples). Furthermore, the Linked Data paradigm is based on the unrestricted dissemination of information by many publishers, as well as the interlinking of Web resources across knowledge bases. It is easy to understand that there are many redundant and duplicate RDFs here; to make our semantic network more efficient and succinct, we must link RDFs together according to certain rules, which is the project's purpose.



\section{The Linking Task}

Actually, in most circumstances, linking RDFs in cross-datasets is quite difficult, and we must rely on Instance Matching (IM) and Link Discovery technologies to do so. Indeed, a few engines, such as Limes and Silk, have achieved this goal. However, because most of them rely on their own link rule languages, it's impossible to apply the same link rules in two or more engines. As a result, users are finding it difficult to use these implementations at the same time, but RDF linking is a fairly common task that must be finished. Furthermore, RDF linking is a computationally intensive job because it requires comparing all resources from one dataset with all resources from another. In other words, for each RDF resource, one or more data comparisons must be performed. So, the way in which link rules expressed, built and processed has a high impact on the performance results, like the language and the strategy used, which can set restrictions to reduce the number of RDF resources to be compared.

For this reason, we create a more general way to linking ontologies based on SPARQL by implementing functions in Apache Jena, which is a SPARQL query engine. So as a software engineer, you don't need to learn other specific language to linking ontology you want, the SPARQL query is enough.








\newpage


------------------------ instructions for the template -----------------------------


%%---------------------------------------------------------
The introduction of the TFM should serve so that teachers who evaluate the work can understand the context in which it is carried out, and the objectives that are set.

This template shows the basic structure of the final TFM memory, as well as some formatting instructions.
The basic schema of a final  TFM memory is:
\begin{itemize}
\item[•] Summary in Spanish and English (max 2 pages each)
\item[•] Content Table
\item[•] Introduction (with TFM's goals)
\item[•] Master’s Project Content
\item[•] Results and conclusions
\item[•] Bibliography (publications used in the study and work development)
\item[•] Annex (optional)
\end{itemize}


In any case, it is the tutor of the TFM who will indicate to his student the final memory structure that best fits the work developed.

Regarding the format, the following guidelines will be followed, which are shown in this template:
\begin{itemize}
\item[•] \textit{Paper size:} DIN A4
\item[•] \textit{Cover Page:} as stated in this template, with indication of university, center, TFM title and author.
\item[•] \textit{Second page:} bibliographic information, including all data of the TFM tutor.
\item[•] \textit{Letter type for text.} Preferably "Bookman Old Style" 11 points. If this is not possible, the recommended alternatives are, in order of preference: "Palatino Linotype", "Garamond" or "Georgia".
\item[•] \textit{Letter type for source code:} “Consolas” or “Roboto mono”
\item[•] \textit{Margins:} upper and lower $3$ cm, left and $2.54$ cm right.
\item[•] \textit{Sections and subsections:} reviewed with decimal numbering after the chapter number. Ej.: subsections 2.3.1.
\item[•] \textit{Page numbers:} always centered on the lower margin, page 1 begins in chapter 1, all sections before chapter 1 in lowercase Roman numeral (i, ii, iii…).
\end{itemize}

\vspace*{1.5cm}
To prepare the final memory of the TFM with this template, follow the steps below:
\begin{enumerate}
\item Download and Install MiKTeX:  \url{https://miktex.org/}
\item Download and install a \LaTeX~ editor, for example Texmaker:\\
\url{https://www.xm1math.net/texmaker/}

\item Edit the file \textbf{secciones/ \_DatosTFM.tex}, which is included in the folder \textbf{secciones} of this template. Fill in all the requested data in said file. Save and close
\item Compile the file
 \textbf{plantilla\_TFM.tex} (can be renamed). A file \textbf{pdf} will be generated as a result.
\item To write the final memory of the TFM you can add and / or modify the files in the \textbf{secciones} as necessary. The result is obtained when compiling the file \textbf{plantilla\_TFM.tex}. 
\end{enumerate}



\section{Source Code example in Python}
\begin{lstlisting}[style=Python]
# -*- coding: utf-8 -*-
import sympy as sy
from sympy.abc import x
\end{lstlisting}
%%---------------------------------------------------------